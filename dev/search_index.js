var documenterSearchIndex = {"docs":
[{"location":"","page":"Home","title":"Home","text":"CurrentModule = Lowess","category":"page"},{"location":"#Lowess","page":"Home","title":"Lowess","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for Lowess.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"","page":"Home","title":"Home","text":"This package includes a pure Julia lowess function, which is an implementation  of the LOWESS smoother (references given at the end of the documentation), along with  a lowess_model function can be used to make corresponding models. ","category":"page"},{"location":"#Tutorial","page":"Home","title":"Tutorial","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"In this section we will go through a simple example to see how our model works. Consider the following function. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Lowess, Plots\nxs = 10 .* rand(100)\nxs = sort(xs)\nys = sin.(xs) .+ 0.5 * rand(100)\n\nmodel = lowess_model(xs, ys, 0.2)\n\nus = range(extrema(xs)...; step = 0.1)\nvs = model(us)\n\nscatter(xs, ys)\nplot!(us, vs, legend=false)","category":"page"},{"location":"","page":"Home","title":"Home","text":"The above code creates some random data points sampled out of a sine curve, with some noise added to the ordinates. A model is created using this data with f = 0.2 (f is the parameter which controls the amount of smoothing). ","category":"page"},{"location":"","page":"Home","title":"Home","text":"us is a vector of abscissas which lie in the range of the abscissas which were passed as input to the model. To get the predicted smooth values for us, we are passing it to the model; the result will be the vector vs, the vector of predicted values. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"Finally, we get the scatter plot of the input points, and the smooth plot using us and vs. The plot looks something like the following. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"(Image: Example Plot)","category":"page"},{"location":"#Comparison-with-[Loess.jl](https://github.com/JuliaStats/Loess.jl)","page":"Home","title":"Comparison with Loess.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Our package is an alternative to Loess.jl, which  exports the more general LOESS predictor. In this section, we benchmark the performance of both the packages on a common dataset and do a comparison in terms of the resources used. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"For our test, we use the example given in the tutorial. For the benchmarks, we use the BenchmarkTools package. In the below code, we benchmark the performance of our package code.","category":"page"},{"location":"","page":"Home","title":"Home","text":"using BenchmarkTools, Lowess\nxs = 10 .* rand(100)\nxs = sort(xs)\nys = sin.(xs) .+ 0.5 * rand(100)\n\n@benchmark begin\n    model = lowess_model(xs, ys, 0.2)\n    us = range(extrema(xs)...; step = 0.1)\n    vs = model(us)\nend","category":"page"},{"location":"","page":"Home","title":"Home","text":"The output of the above performance benchmark is given below.","category":"page"},{"location":"","page":"Home","title":"Home","text":"BenchmarkTools.Trial: 10000 samples with 1 evaluation.\nRange (min … max):  47.797 μs …  1.420 ms  ┊ GC (min … max): 0.00% … 95.55%\nTime  (median):     58.236 μs              ┊ GC (median):    0.00%\nTime  (mean ± σ):   58.842 μs ± 15.965 μs  ┊ GC (mean ± σ):  0.23% ±  0.96%","category":"page"},{"location":"","page":"Home","title":"Home","text":"The exact same benchmarking code, but with Loess.jl, is given below. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Loess\n@benchmark begin\n    model = loess(xs, ys, span=0.5)\n    us = range(extrema(xs)...; step = 0.1)\n    vs = predict(model, us)\nend","category":"page"},{"location":"","page":"Home","title":"Home","text":"The output of the above benchmarking test is given below.","category":"page"},{"location":"","page":"Home","title":"Home","text":"BenchmarkTools.Trial: 6040 samples with 1 evaluation.\n Range (min … max):  574.949 μs …   3.726 ms  ┊ GC (min … max):  0.00% … 64.41%\n Time  (median):     693.659 μs               ┊ GC (median):     0.00%\n Time  (mean ± σ):   825.584 μs ± 515.006 μs  ┊ GC (mean ± σ):  16.85% ± 18.90%","category":"page"},{"location":"","page":"Home","title":"Home","text":"For the above test, our package code runs much faster compared to Loess.jl.","category":"page"},{"location":"#References","page":"Home","title":"References","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"[1] Cleveland, W. S. (1979). Robust locally weighted regression and smoothing scatterplots. Journal of the American statistical association, 74(368), 829-836. DOI: 10.1080/01621459.1979.10481038","category":"page"},{"location":"","page":"Home","title":"Home","text":"[2] Cleveland, W. S., & Devlin, S. J. (1988). Locally weighted regression: an approach to regression analysis by local fitting. Journal of the American statistical association, 83(403), 596-610. DOI: 10.1080/01621459.1988.10478639","category":"page"},{"location":"","page":"Home","title":"Home","text":"[3] Cleveland, W. S., & Grosse, E. (1991). Computational methods for local regression. Statistics and computing, 1(1), 47-62. DOI: 10.1007/BF01890836","category":"page"},{"location":"#API-Reference","page":"Home","title":"API Reference","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Modules = [Lowess]","category":"page"},{"location":"#Lowess.lowess-Union{Tuple{T}, Tuple{AbstractVector{T}, AbstractVector{T}}, Tuple{AbstractVector{T}, AbstractVector{T}, T}, Tuple{AbstractVector{T}, AbstractVector{T}, T, Integer}, Tuple{AbstractVector{T}, AbstractVector{T}, T, Integer, T}} where T<:AbstractFloat","page":"Home","title":"Lowess.lowess","text":"lowess(x, y, f = 2/3, nsteps = 3, delta = 0.01*(maximum(x) - minimum(x)))\n\nCompute the smooth of a scatterplot of y against x using robust locally weighted regression. Input vectors x and y must contain either integers or floats. Parameters f and delta must be of type T, where T <: AbstractFloat. Returns a vector ys; ys[i] is the fitted value at x[i]. To get the smooth plot, ys must be plotted against x. \n\nArguments\n\nx::Vector: Abscissas of the points on the scatterplot. x must be ordered.\ny::Vector: Ordinates of the points in the scatterplot. \nf::T: The amount of smoothing. \nnsteps::Integer: Number of iterations in the robust fit.\ndelta::T: A nonnegative parameter which may be used to save computations.\n\nExample\n\nusing Lowess, Plots\nx = sort(10 .* rand(100))\ny = sin.(x) .+ 0.5 * rand(100)\nys = lowess(x, y, 0.2)\nscatter(x, y)\nplot!(x, ys)\n\n\n\n\n\n","category":"method"},{"location":"#Lowess.lowess_model","page":"Home","title":"Lowess.lowess_model","text":"lowess_model(xs, ys, f = 2/3, nsteps = 3, delta = 0.01*(maximum(xs) - minimum(xs)))\n\nReturn a lowess model which can be used to predict the ordinate corresponding to a new abscissa. Has the same arguments as lowess.\n\nExample\n\nusing Lowess, Plots\nxs = 10 .* rand(100)\nxs = sort(xs)\nys = sin.(xs) .+ 0.5 * rand(100)\n\nmodel = lowess_model(xs, ys, 0.2)\n\nus = range(extrema(xs)...; step = 0.1)\nvs = model(us)\n\nscatter(xs, ys)\nplot!(us, vs, legend=false)\n\n\n\n\n\n","category":"function"}]
}
